{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " This command clones a Git repository from GitHub to a local directory named \"Big-Data\".\n"
      ],
      "metadata": {
        "id": "foDh7jleFYBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the Repository\n",
        "!git clone https://FaridRash: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@github.com/FaridRash/HW-SW-B.git Big-Data"
      ],
      "metadata": {
        "id": "kXunvqTTCLDe",
        "outputId": "4449e06b-2f9d-4b22-f310-d74fe46116cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Big-Data'...\n",
            "remote: Enumerating objects: 203, done.\u001b[K\n",
            "remote: Counting objects: 100% (203/203), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 203 (delta 95), reused 13 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (203/203), 8.61 MiB | 8.40 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit -q\n",
        "!pip install -g ngrok\n",
        "!pip install -g localtunnel\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "yBNfs9ni605G",
        "outputId": "925ab0e6-df0f-4f9f-d8dc-63db9d3589aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -g\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -g\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pgTTR6tkjYpm",
        "outputId": "da5c5c5e-6907-4f85-c2d3-23e5423cc3c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import plotly.figure_factory as ff\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import streamlit as st\n",
        "import pyngrok\n",
        "import io\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset from the specified path into a pandas DataFrame\n",
        "data = pd.read_csv('/content/Big-Data/Data/xAPI-Edu-Data.csv')\n",
        "\n",
        "# Display the first 10 rows of the DataFrame to get an overview of the data\n",
        "st.title('Displaying a Dataset in Streamlit')\n",
        "st.write(data)\n",
        "data.head(10)\n",
        "\n",
        "# Print the shape of the DataFrame to know the number of rows and columns\n",
        "data_shape = data.shape\n",
        "st.title('dataset Shape:')\n",
        "st.write(data_shape)\n",
        "\n",
        "# Display a concise summary of the DataFrame, including data types and non-null counts\n",
        "st.title('Show Dataset information:')\n",
        "# Capture the output of data.info()\n",
        "buffer = io.StringIO()\n",
        "data.info(buf=buffer)\n",
        "info_str = buffer.getvalue()\n",
        "\n",
        "# Display the output in Streamlit\n",
        "st.text(info_str)\n",
        "\n",
        "# Generate descriptive statistics for the numerical columns in the DataFrame\n",
        "st.title('Describe Datast:')\n",
        "data_describe = data.describe()\n",
        "st.write(data_describe)\n",
        "\n",
        "# Extract the column names into a list\n",
        "columns_list = data.columns.tolist()\n",
        "st.title('show the column name:')\n",
        "# Enumerate through the list of column names and print each with its index\n",
        "for index, column_name in enumerate(columns_list):\n",
        "    st.write(f\"{index + 1}. {column_name}\")\n",
        "\n",
        "# Calculate the number of missing values in each column\n",
        "missing_data = data.isnull().sum()\n",
        "st.title('the missing values for each column:')\n",
        "# Print the missing values for each column\n",
        "st.write(missing_data)\n",
        "st.title('nun values heatmap:')\n",
        "# Create the heatmap\n",
        "fig, ax = plt.subplots()\n",
        "sns.heatmap(data.isnull(), cbar=False, yticklabels=False, ax=ax)\n",
        "\n",
        "# Display the heatmap in Streamlit\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Check if there are any duplicate rows and count the number of duplicate rows\n",
        "are_duplicates = data.duplicated().any()\n",
        "num_duplicates = data.duplicated().sum()\n",
        "st.title('check the duplicated data:')\n",
        "# Print the results\n",
        "st.write('are duplicates:',are_duplicates)\n",
        "st.write('number of duplicates:', num_duplicates)\n",
        "\n",
        "# Remove duplicate rows from the DataFrame\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "# Recheck if there are any duplicate rows and count the number of duplicate rows after removing them\n",
        "\n",
        "are_duplicates_after_removal = data.duplicated().any()\n",
        "num_duplicates_after_removal = data.duplicated().sum()\n",
        "\n",
        "# Print the results to confirm duplicates have been removed\n",
        "st.title('check the duplicated data:')\n",
        "# Print the results\n",
        "st.write('are duplicates:',are_duplicates_after_removal)\n",
        "st.write('number of duplicates:', num_duplicates_after_removal)\n",
        "#------------------------------------------------------\n",
        "st.title('visualize main columns:')\n",
        "# Create the boxplot\n",
        "fig, ax = plt.subplots()\n",
        "sns.boxplot(data=data[['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion']], ax=ax)\n",
        "\n",
        "# Display the plot in Streamlit\n",
        "st.pyplot(fig)\n",
        "#---------------------------------------------------\n",
        "st.title('create  histograms:')\n",
        "# Create the histograms\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
        "data[['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion']].hist(ax=axes)\n",
        "st.pyplot(fig)\n",
        "#*------------------------\n",
        "st.title('Bar plots for categorical data')\n",
        "categorical_cols = ['gender', 'NationalITy', 'PlaceofBirth', 'StageID', 'GradeID', 'SectionID', 'Topic', 'Semester', 'Relation', 'ParentAnsweringSurvey', 'ParentschoolSatisfaction', 'StudentAbsenceDays', 'Class']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.countplot(x=col, data=data)\n",
        "    plt.title(col)\n",
        "    st.pyplot(plt)\n",
        "#Bar plots for categorical data\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Select columns with object data type\n",
        "\n",
        "object_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
        "# Iterate over each object column and print the unique values\n",
        "st.title(' Iterate over each object column and print the unique values:')\n",
        "for col_index in range(len(object_columns)):\n",
        "    col_name = object_columns[col_index]\n",
        "    unique_values = data[col_name].unique()\n",
        "    st.write(f\"Unique values for column '{col_name}':\")\n",
        "    for value in unique_values:\n",
        "        st.write(value)\n",
        "    st.write()\n",
        "\n",
        "\n",
        "def get_uniques_alternative(df, columns):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of unique values for specified columns in the DataFrame.\n",
        "\n",
        "    :param df: pandas DataFrame\n",
        "    :param columns: list of column names\n",
        "    :return: dictionary with column names as keys and lists of unique values as values\n",
        "    \"\"\"\n",
        "    unique_values_dict = {}\n",
        "    for column in columns:\n",
        "        unique_values_dict[column] = df[column].unique().tolist()\n",
        "    return unique_values_dict\n",
        "\n",
        "\n",
        "def get_categorical_columns_alternative(df):\n",
        "    \"\"\"\n",
        "    Returns a list of column names that have a data type of 'object'.\n",
        "\n",
        "    :param df: pandas DataFrame\n",
        "    :return: list of categorical column names\n",
        "    \"\"\"\n",
        "    categorical_columns = []\n",
        "    for column in df.columns:\n",
        "        if df.dtypes[column] == 'object':\n",
        "            categorical_columns.append(column)\n",
        "    return categorical_columns\n",
        "\n",
        "\n",
        "# Get unique values for all categorical columns in the DataFrame\n",
        "unique_values_dict = get_uniques_alternative(data, get_categorical_columns_alternative(data))\n",
        "st.title('Display the dictionary of unique values')\n",
        "# Display the dictionary of unique values\n",
        "st.write(unique_values_dict)\n",
        "\n",
        "\n",
        "st.title('Display the column names of the DataFrame:')\n",
        "# Display the column names of the DataFrame\n",
        "st.write(data.columns)\n",
        "\n",
        "\n",
        "st.title(\"Calculate and print the normalized value counts of 'gender' grouped by 'Class'\")\n",
        "# Calculate and print the normalized value counts of 'gender' grouped by 'Class'\n",
        "st.write(data.groupby(['Class'])['gender'].value_counts(normalize=True), '\\n', '\\n', '\\n')\n",
        "\n",
        "st.title(\"Calculate and print the normalized value counts of 'NationalITy' grouped by 'Class'\")\n",
        "# Calculate and print the normalized value counts of 'NationalITy' grouped by 'Class'\n",
        "st.write(data.groupby(['Class'])['NationalITy'].value_counts(normalize=True), '\\n', '\\n', '\\n')\n",
        "\n",
        "st.title(\"Calculate and print the normalized value counts of 'PlaceofBirth' grouped by 'Class'\")\n",
        "# Calculate and print the normalized value counts of 'PlaceofBirth' grouped by 'Class'\n",
        "st.write(data.groupby(['Class'])['PlaceofBirth'].value_counts(normalize=True), '\\n', '\\n', '\\n')\n",
        "\n",
        "st.title(\"Calculate and print the normalized value counts of 'StageID' grouped by 'Class'\")\n",
        "# Calculate and print the normalized value counts of 'StageID' grouped by 'Class'\n",
        "st.write(data.groupby(['Class'])['StageID'].value_counts(normalize=True), '\\n', '\\n', '\\n')\n",
        "\n",
        "\n",
        "# Categorize the features into binary, ordinal, and nominal categories\n",
        "binary_features = ['gender', 'Semester', 'Relation', 'ParentAnsweringSurvey', 'ParentschoolSatisfaction', 'StudentAbsenceDays']\n",
        "ordinal_features = ['StageID', 'GradeID']\n",
        "nominal_features = ['NationalITy', 'PlaceofBirth', 'SectionID', 'Topic']\n",
        "\n",
        "# Specify the target column\n",
        "target_column = 'Class'\n",
        "\n",
        "\n",
        "# Define the positive values for binary encoding of binary features\n",
        "binary_positive_values = ['M', 'S', 'Father', 'Yes', 'Good', 'Above-7']\n",
        "\n",
        "\n",
        "# Define the ordering for the 'StageID' ordinal feature\n",
        "stage_ordering = ['lowerlevel', 'MiddleSchool', 'HighSchool']\n",
        "\n",
        "# Define the ordering for the 'GradeID' ordinal feature\n",
        "grade_ordering = ['G-02', 'G-04', 'G-05', 'G-06', 'G-07', 'G-08', 'G-09', 'G-10', 'G-11', 'G-12']\n",
        "\n",
        "\n",
        "# Define prefixes for nominal features to be used in encoding\n",
        "nominal_prefixes = ['N', 'B', 'S', 'T']\n",
        "\n",
        "\n",
        "# Function to perform binary encoding on a specified column\n",
        "def binary_encode_alternative(df, column, positive_value):\n",
        "    df = df.copy()\n",
        "    df[column] = df[column].map(lambda x: 1 if x == positive_value else 0)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Function to perform ordinal encoding on a specified column\n",
        "def ordinal_encode_alternative(df, column, ordering):\n",
        "    df = df.copy()\n",
        "    df[column] = df[column].map(ordering.index)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Function to perform one-hot encoding on a specified column\n",
        "def onehot_encode_alternative(df, column, prefix):\n",
        "    df = df.copy()\n",
        "    dummies = pd.get_dummies(df[column], prefix=prefix).astype(int)\n",
        "    df = df.join(dummies).drop(column, axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Apply binary encoding to each feature in the binary_features list\n",
        "for feature, positive_value in zip(binary_features, binary_positive_values):\n",
        "    data = binary_encode_alternative(data, feature, positive_value)\n",
        "\n",
        "# Apply one-hot encoding to each feature in the nominal_features list\n",
        "for feature, prefix in zip(nominal_features, nominal_prefixes):\n",
        "    data = onehot_encode_alternative(data, feature, prefix)\n",
        "\n",
        "\n",
        "# Apply ordinal encoding to the 'StageID' column\n",
        "data = ordinal_encode_alternative(data, 'StageID', stage_ordering)\n",
        "\n",
        "# Apply ordinal encoding to the 'GradeID' column\n",
        "data = ordinal_encode_alternative(data, 'GradeID', grade_ordering)\n",
        "\n",
        "# Define the ordering for the target column 'Class'\n",
        "target_ordering = ['L', 'M', 'H']\n",
        "\n",
        "# Apply ordinal encoding to the target column\n",
        "encoded_data = ordinal_encode_alternative(data, target_column, target_ordering)\n",
        "\n",
        "\n",
        "# Display the first 10 rows of the encoded DataFrame to verify the transformations\n",
        "st.title('Display the first 10 rows of the encoded DataFrame to verify the transformations:')\n",
        "st.write(encoded_data)\n",
        "\n",
        "\n",
        "# Print the shape of the DataFrame after encoding to verify the dimensions\n",
        "st.title('Shape of encoded data:')\n",
        "st.write(data.shape)\n",
        "\n",
        "\n",
        "# Extract the column names from the encoded DataFrame into a list\n",
        "columns_list = encoded_data.columns.tolist()\n",
        "st.title('columns of encoded data:')\n",
        "# Enumerate through the list of column names and print each with its index\n",
        "for index, column_name in enumerate(columns_list):\n",
        "    st.write(f\"{index + 1}. {column_name}\")\n",
        "\n",
        "# Calculate the number of missing values in each column of the encoded DataFrame\n",
        "missing_values = encoded_data.isnull().sum()\n",
        "st.title('check the missing calues of encoded data')\n",
        "# Print the missing values for each column to identify any issues\n",
        "st.write(\"Missing Values:\\n\", missing_values)\n",
        "\n",
        "\n",
        "st.title('display the encoded data:')\n",
        "# Display the first few rows of the encoded DataFrame to verify the changes\n",
        "st.write(encoded_data)\n",
        "\n",
        "st.title('Plot a box plot to visualize the distribution of features before standardization:')\n",
        "# Select relevant features for modeling by dropping the target column 'Class'\n",
        "features = encoded_data.drop(['Class'], axis=1)\n",
        "\n",
        "# Plot a box plot to visualize the distribution of features before standardization\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=features)\n",
        "plt.title('Box Plot Before Standardization')\n",
        "plt.xticks(rotation=90)\n",
        "# Use Streamlit to display the plot\n",
        "st.pyplot(plt)\n",
        "\n",
        "st.title('Plot a box plot to visualize the distribution of features after standardization:')\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Create a DataFrame with the standardized features\n",
        "scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "\n",
        "# Plot a box plot to visualize the distribution of features after standardization\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=scaled_features_df)\n",
        "plt.title('Box Plot After Standardization')\n",
        "plt.xticks(rotation=90)\n",
        "st.pyplot(plt)\n",
        "\n",
        "st.title('Plot data after capping outliers:')\n",
        "# Function to cap outliers\n",
        "def cap_outliers(data):\n",
        "    capped_data = data.copy()\n",
        "    for col in capped_data.columns:\n",
        "        upper_limit = capped_data[col].quantile(0.80)  # Using 80th percentile\n",
        "        lower_limit = capped_data[col].quantile(0.00)  # Using 0th percentile (minimum value)\n",
        "        capped_data[col] = np.where(capped_data[col] > upper_limit, upper_limit, capped_data[col])\n",
        "        capped_data[col] = np.where(capped_data[col] < lower_limit, lower_limit, capped_data[col])\n",
        "    return capped_data\n",
        "\n",
        "# Cap outliers in the standardized DataFrame\n",
        "data_capped = cap_outliers(scaled_features_df)\n",
        "\n",
        "# Function to plot data\n",
        "def plot_data(data, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.boxplot(data.values, vert=False, patch_artist=True)\n",
        "    plt.title(title)\n",
        "    st.pyplot(plt)\n",
        "# Plot data after capping outliers\n",
        "st.write('After Capping Outliers',data_capped)\n",
        "\n",
        "st.title('Display final VIF values:')\n",
        "# List of columns to remove due to low VIF values\n",
        "low_vif_columns = [\n",
        "    'GradeID', 'N_Iran', 'N_Egypt', 'N_Iraq', 'N_KW', 'N_Lybia', 'N_Morocco',\n",
        "    'N_Palestine', 'N_SaudiArabia', 'N_Syria', 'N_Tunis', 'N_USA', 'N_lebanon',\n",
        "    'N_venzuela', 'B_Iran', 'B_Egypt',  'B_SaudiArabia', 'B_Syria',\n",
        "    'B_USA', 'B_venzuela', 'B_Iraq', 'B_Morocco', 'T_History', 'B_Palestine',\n",
        "    'B_lebanon', 'S_C', 'B_Tunis', 'T_Arabic', 'T_Biology', 'T_Chemistry',\n",
        "    'T_English', 'T_French', 'T_Geology', 'T_IT', 'T_Math', 'T_Quran', 'T_Science',\n",
        "    'T_Spanish'\n",
        "]\n",
        "\n",
        "# Remove the columns with low VIF values\n",
        "final_features_df = data_capped.drop(columns=low_vif_columns)\n",
        "\n",
        "# Recalculate VIF for the reduced dataset\n",
        "vif_final_data = pd.DataFrame()\n",
        "vif_final_data[\"feature\"] = final_features_df.columns\n",
        "vif_final_data[\"VIF\"] = [variance_inflation_factor(final_features_df.values, i) for i in range(len(final_features_df.columns))]\n",
        "\n",
        "# Display final VIF values\n",
        "st.write(vif_final_data)\n",
        "\n",
        "st.title('Check for missing values in the VIF DataFrame:')\n",
        "# Check for missing values in the VIF DataFrame\n",
        "missing_values = vif_final_data.isnull().sum()\n",
        "\n",
        "# Print the number of missing values to ensure data integrity\n",
        "st.write(\"Missing Values:\\n\", missing_values)\n",
        "\n",
        "st.title('Display the  rows of the DataFrame after dropping specified columns:')\n",
        "# Remove specified columns to reduce multicollinearity\n",
        "vif_droped_data = data_capped.drop([\n",
        "    'GradeID', 'N_Iran', 'N_Egypt', 'N_Iraq', 'N_KW', 'N_Lybia', 'N_Morocco',\n",
        "    'N_Palestine', 'N_SaudiArabia', 'N_Syria', 'N_Tunis', 'N_USA', 'N_lebanon',\n",
        "    'N_venzuela', 'B_Iran', 'B_Egypt', 'B_SaudiArabia', 'B_Syria',\n",
        "    'B_USA', 'B_venzuela', 'B_Iraq', 'B_Morocco', 'T_History', 'B_Palestine',\n",
        "    'B_lebanon', 'S_C', 'B_Tunis', 'T_Arabic', 'T_Biology', 'T_Chemistry',\n",
        "    'T_English', 'T_French', 'T_Geology', 'T_IT', 'T_Math', 'T_Quran', 'T_Science',\n",
        "    'T_Spanish'], axis=1)\n",
        "\n",
        "# Display the first few rows of the DataFrame after dropping specified columns\n",
        "st.write(vif_droped_data)\n",
        "\n",
        "# Add the target column 'Class' back to the DataFrame after dropping specified columns\n",
        "vif_droped_data['Class'] = encoded_data['Class']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "st.title('show the heatmap:')\n",
        "# Identify numerical columns in the DataFrame\n",
        "numerical_cols = [col for col in vif_droped_data.columns if vif_droped_data[col].dtype != 'object']\n",
        "\n",
        "# Calculate the correlation matrix for the numerical columns\n",
        "correlation_matrix = vif_droped_data[numerical_cols].corr()\n",
        "\n",
        "# Create an annotated heatmap for the correlation matrix\n",
        "fig = ff.create_annotated_heatmap(\n",
        "    z=correlation_matrix.to_numpy(),\n",
        "    x=correlation_matrix.columns.tolist(),\n",
        "    y=correlation_matrix.columns.tolist(),\n",
        "    colorscale='Viridis',\n",
        "    reversescale=True,\n",
        "    annotation_text=correlation_matrix.round(2).values,\n",
        "    font_colors=['white', 'black'],\n",
        ")\n",
        "\n",
        "# Update the layout of the heatmap for better visualization\n",
        "fig.update_layout(\n",
        "    title='Correlation Matrix',\n",
        "    xaxis_title='Features',\n",
        "    yaxis_title='Features',\n",
        "    yaxis_autorange='reversed',\n",
        "    font=dict(size=10),\n",
        "    width=1500,\n",
        "    height=1500\n",
        ")\n",
        "\n",
        "# Show the heatmap\n",
        "st.plotly_chart(fig)\n",
        "\n",
        "# Remove the 'B_Lybia' column from the DataFrame\n",
        "vif_droped_data = vif_droped_data.drop(['B_Lybia'], axis=1)\n",
        "\n",
        "st.title('Update the layout of the heatmap for better visualization:')\n",
        "# Identify numerical columns in the DataFrame\n",
        "numerical_cols = [col for col in vif_droped_data.columns if vif_droped_data[col].dtype != 'object']\n",
        "\n",
        "# Calculate the correlation matrix for the numerical columns\n",
        "correlation_matrix = vif_droped_data[numerical_cols].corr()\n",
        "\n",
        "# Create an annotated heatmap for the correlation matrix\n",
        "fig = ff.create_annotated_heatmap(\n",
        "    z=correlation_matrix.to_numpy(),\n",
        "    x=correlation_matrix.columns.tolist(),\n",
        "    y=correlation_matrix.columns.tolist(),\n",
        "    colorscale='Viridis',\n",
        "    reversescale=True,\n",
        "    annotation_text=correlation_matrix.round(2).values,\n",
        "    font_colors=['white', 'black'],\n",
        ")\n",
        "\n",
        "# Update the layout of the heatmap for better visualization\n",
        "fig.update_layout(\n",
        "    title='Correlation Matrix',\n",
        "    xaxis_title='Features',\n",
        "    yaxis_title='Features',\n",
        "    yaxis_autorange='reversed',\n",
        "    font=dict(size=10),\n",
        "    width=1500,\n",
        "    height=1500\n",
        ")\n",
        "\n",
        "# Show the heatmap\n",
        "st.plotly_chart(fig)\n",
        "\n",
        "st.title('Display the column names of the DataFrame to verify the current set of features:')\n",
        "# Display the column names of the DataFrame to verify the current set of features\n",
        "st.write(\"Current set of features:\", vif_droped_data.columns.tolist())\n",
        "\n",
        "st.title('Plot the WCSS values to use the Elbow Method:')\n",
        "# Define the feature columns and the target column\n",
        "feature_columns = ['gender', 'StageID', 'Semester', 'Relation', 'raisedhands',\n",
        "                   'VisITedResources', 'AnnouncementsView', 'Discussion',\n",
        "                   'ParentAnsweringSurvey', 'ParentschoolSatisfaction',\n",
        "                   'StudentAbsenceDays', 'N_Jordan', 'B_Jordan', 'B_KuwaIT', 'S_A', 'S_B']\n",
        "target_column = 'Class'\n",
        "\n",
        "# Select only the feature columns for clustering\n",
        "X = vif_droped_data[feature_columns]\n",
        "\n",
        "# Calculate WCSS for different number of clusters\n",
        "wcss = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the WCSS values to use the Elbow Method\n",
        "plt.plot(range(1, 11), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('WCSS')  # Within-Cluster Sum of Squares\n",
        "st.pyplot(plt)\n",
        "\n",
        "#--------------------------------------------------------------------\n",
        "st.title('Display the DataFrame with cluster labels:')\n",
        "# Define the feature columns and the target column\n",
        "feature_columns = ['gender', 'StageID', 'Semester', 'Relation', 'raisedhands',\n",
        "                   'VisITedResources', 'AnnouncementsView', 'Discussion',\n",
        "                   'ParentAnsweringSurvey', 'ParentschoolSatisfaction',\n",
        "                   'StudentAbsenceDays', 'N_Jordan', 'B_Jordan', 'B_KuwaIT', 'S_A', 'S_B']\n",
        "target_column = 'Class'\n",
        "\n",
        "# Select only the feature columns for clustering\n",
        "X = vif_droped_data[feature_columns]\n",
        "\n",
        "# Optimal number of clusters determined from Elbow Method\n",
        "optimal_clusters = 3  # Example, replace with the number you found optimal\n",
        "\n",
        "# Fit the K-Means model with the optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=optimal_clusters, init='k-means++', random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Predict the clusters\n",
        "clusters = kmeans.predict(X)\n",
        "\n",
        "# Add the cluster labels to the original DataFrame\n",
        "vif_droped_data['Cluster'] = clusters\n",
        "\n",
        "# Display the DataFrame with cluster labels\n",
        "st.write(vif_droped_data)\n",
        "\n",
        "# Reduce to 2 dimensions for visualization using PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced_X = pca.fit_transform(X)\n",
        "st.title('Plot the clusters:')\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(reduced_X[:, 0], reduced_X[:, 1], c=clusters, cmap='viridis')\n",
        "plt.title('K-Means Clusters')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "st.pyplot(plt)\n",
        "\n",
        "#--------------------------------------------\n",
        "st.title('Show the contribution of each feature to each principal component:')\n",
        "# Fit PCA model to the selected features\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "\n",
        "# Extract the principal components\n",
        "principal_components = pca.components_\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = X.columns\n",
        "\n",
        "# Initialize a list to store the results\n",
        "results = []\n",
        "\n",
        "# Print and store the contribution of each feature to each principal component\n",
        "for i, component in enumerate(principal_components):\n",
        "    st.write(f\"Principal Component {i + 1}:\")\n",
        "    for j, feature in enumerate(feature_names):\n",
        "        st.write(f\"\\t{feature}: {component[j]:.4f}\")\n",
        "        results.append([f\"Principal Component {i + 1}\", feature, component[j]])\n",
        "\n",
        "#-------------------------------------------------------\n",
        "st.title(' Show the  rows of the DataFrame to verify the contributions:')\n",
        "# Create a DataFrame from the results of the PCA component contributions\n",
        "results_df = pd.DataFrame(results, columns=[\"Component\", \"Feature\", \"Value\"])\n",
        "\n",
        "# Print the first 20 rows of the DataFrame to verify the contributions\n",
        "st.write(results_df)\n",
        "#----------------------------------------\n",
        "st.title('Display the DataFrame to verify the contributions:')\n",
        "# Pivot the DataFrame to show contributions of each feature to each principal component\n",
        "pivot_df = results_df.pivot(index='Feature', columns='Component', values='Value')\n",
        "\n",
        "# Reset index to make 'Feature' a column again\n",
        "pivot_df.reset_index(inplace=True)\n",
        "\n",
        "# Display the DataFrame to verify the contributions\n",
        "st.write(pivot_df)\n",
        "#-------------------------------\n",
        "st.title('Sort and display the features by their contribution to each principal component:')\n",
        "# Sort and display the features by their contribution to each principal component\n",
        "for component in pivot_df.columns[1:]:  # Excluding the 'Feature' column\n",
        "    sorted_df = pivot_df[['Feature', component]].sort_values(by=component, ascending=False)\n",
        "    st.write(f\"Sorted features for {component}:\")\n",
        "    st.write(sorted_df)\n",
        "    st.markdown(\"<br>\", unsafe_allow_html=True)\n",
        "\n",
        "#-------------------------------------------------\n",
        "st.title('Plot the sorted feature contributions:')\n",
        "# Iterate through each principal component\n",
        "for component in pivot_df.columns[1:]:\n",
        "    # Sort features by their contribution to the current principal component\n",
        "    sorted_df = pivot_df[['Feature', component]].sort_values(by=component, ascending=False)\n",
        "\n",
        "    # Plot the sorted feature contributions\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.barh(sorted_df['Feature'], sorted_df[component], color='skyblue')\n",
        "    plt.xlabel('Value')\n",
        "    plt.title(f'Sorted features for {component}')\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to have the highest value at the top\n",
        "    st.pyplot(plt)\n",
        "#----------------------------------------------------------\n",
        "\n",
        "st.title('Display mean and standard deviation for each cluster:')\n",
        "# Calculate the mean and standard deviation for each cluster\n",
        "cluster_stats = vif_droped_data.groupby('Cluster').agg(['mean', 'std'])\n",
        "\n",
        "# Display the results\n",
        "st.write(cluster_stats)\n",
        "#---------------------------------------------------------\n",
        "st.title('Mean and Standard Deviation  values for each columns:')\n",
        "# Extract feature columns from the hierarchical columns of cluster_stats\n",
        "feature_columns = cluster_stats.columns.levels[0]\n",
        "\n",
        "# Number of clusters (shape[0] gives the number of rows, which corresponds to clusters)\n",
        "n_clusters = cluster_stats.shape[0]\n",
        "\n",
        "# Plotting setup\n",
        "n_features = len(feature_columns)\n",
        "fig, axes = plt.subplots(n_features, 1, figsize=(10, n_features * 4), sharex=True)\n",
        "\n",
        "# Iterate over each feature column\n",
        "for i, feature in enumerate(feature_columns):\n",
        "    # Extract mean and standard deviation values for the current feature across clusters\n",
        "    means = cluster_stats[feature]['mean']\n",
        "    stds = cluster_stats[feature]['std']\n",
        "\n",
        "    # Create a bar plot for mean with error bars representing standard deviation\n",
        "    axes[i].bar(range(n_clusters), means, yerr=stds, align='center', alpha=0.7, ecolor='black', capsize=10)\n",
        "\n",
        "    # Set title and labels for each subplot\n",
        "    axes[i].set_title(f'Mean and Standard Deviation of {feature}')\n",
        "    axes[i].set_ylabel('Value')\n",
        "    axes[i].set_xticks(range(n_clusters))\n",
        "    axes[i].set_xticklabels([f'Cluster {x}' for x in range(n_clusters)])\n",
        "\n",
        "# Set common xlabel for all subplots\n",
        "plt.xlabel('Cluster')\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "plt.tight_layout()\n",
        "st.pyplot(plt)\n",
        "#---------------------------------------------------\n",
        "st.title('Heatmap of Means for Each Feature by Cluster:')\n",
        "# Extract mean values from cluster_stats using cross-section (xs)\n",
        "mean_cluster_stats = cluster_stats.xs('mean', axis=1, level=1)\n",
        "\n",
        "# Transpose the DataFrame to switch rows (clusters) and columns (features) for better visualization\n",
        "mean_cluster_stats_transposed = mean_cluster_stats.T\n",
        "\n",
        "# Plotting the heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(mean_cluster_stats_transposed, annot=True, cmap='coolwarm')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title('Heatmap of Means for Each Feature by Cluster')\n",
        "plt.xlabel('Clusters')\n",
        "plt.ylabel('Features')\n",
        "\n",
        "# Display the plot\n",
        "st.pyplot(plt)\n",
        "#----------------------------------------------------------\n",
        "st.title('Box Plot of Features by Cluster:')\n",
        "# Melt the DataFrame to long format for seaborn\n",
        "melted_data = vif_droped_data.melt(id_vars=['Cluster'], value_vars=feature_columns, var_name='Feature', value_name='Value')\n",
        "\n",
        "# Plotting the box plot\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(x='Feature', y='Value', hue='Cluster', data=melted_data)\n",
        "plt.title('Box Plot of Features by Cluster')\n",
        "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility if necessary\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Value')\n",
        "plt.legend(title='Cluster')  # Add legend with cluster labels\n",
        "plt.tight_layout()\n",
        "st.pyplot(plt)\n",
        "#-----------------------------------------------\n",
        "st.title('Check for missing values in each column of vif_droped_data:')\n",
        "# Check for missing values in each column of vif_droped_data\n",
        "missing_values = vif_droped_data.isnull().sum()\n",
        "\n",
        "# Print the missing values counts\n",
        "st.write(\"Missing Values:\\n\", missing_values)\n",
        "#------------------------------------------------------\n",
        "# Drop rows with NaN values in the target column\n",
        "vif_droped_data = vif_droped_data.dropna(subset=['Class'])\n",
        "#--------------------------------------------------------\n",
        "st.title('Check for missing values in each column:')\n",
        "# Check for missing values in each column\n",
        "missing_values = vif_droped_data.isnull().sum()\n",
        "\n",
        "# Print the missing values count\n",
        "st.write(\"Missing Values:\\n\", missing_values)\n",
        "#-------------------------------------------------------------\n",
        "st.title('Result of Feature Importances from Random Forest')\n",
        "\n",
        "# Define feature columns and target column\n",
        "feature_columns = ['gender', 'StageID', 'Semester', 'Relation', 'raisedhands',\n",
        "                   'VisITedResources', 'AnnouncementsView', 'Discussion',\n",
        "                   'ParentAnsweringSurvey', 'ParentschoolSatisfaction',\n",
        "                   'StudentAbsenceDays', 'N_Jordan', 'B_Jordan', 'B_KuwaIT', 'S_A', 'S_B']\n",
        "\n",
        "# Define feature columns and target column for \"Cluster\"\n",
        "target_column = 'Cluster'\n",
        "\n",
        "# Select features (X) and target (y)\n",
        "X = vif_droped_data[feature_columns]\n",
        "y = vif_droped_data[target_column]\n",
        "\n",
        "# Split data into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "st.write(f'Accuracy: {accuracy}')\n",
        "st.write('Classification Report:')\n",
        "st.write(classification_rep)\n",
        "st.write('Confusion Matrix:')\n",
        "st.write(conf_matrix)\n",
        "\n",
        "# Optional: Feature Importance\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate feature importances\n",
        "feature_importances = pd.DataFrame(rf_clf.feature_importances_,\n",
        "                                   index=feature_columns,\n",
        "                                   columns=['Importance']).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Print feature importances\n",
        "st.write('Feature Importances:')\n",
        "st.write(feature_importances)\n",
        "\n",
        "# Visualize Feature Importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importances.index, feature_importances['Importance'], color='skyblue')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importances from Random Forest')\n",
        "plt.gca().invert_yaxis()\n",
        "st.pyplot(plt)\n",
        "#-----------------------------------------\n",
        "st.title('Classification Report for Decision Tree:')\n",
        "# Import Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "classification_rep_dt = classification_report(y_test, y_pred_dt)\n",
        "conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "# Print evaluation metrics\n",
        "st.write(f'Accuracy (Decision Tree): {accuracy_dt}')\n",
        "st.write('Classification Report (Decision Tree):')\n",
        "st.write(classification_rep_dt)\n",
        "st.write('Confusion Matrix (Decision Tree):')\n",
        "st.write(conf_matrix_dt)\n",
        "\n",
        "# Optional: Feature Importance\n",
        "feature_importances_dt = pd.DataFrame(dt_clf.feature_importances_,\n",
        "                                       index=feature_columns,\n",
        "                                       columns=['Importance']).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Print feature importances\n",
        "st.write('Feature Importances (Decision Tree):')\n",
        "st.write(feature_importances_dt)\n",
        "\n",
        "# Visualize Feature Importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importances_dt.index, feature_importances_dt['Importance'], color='lightcoral')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importances from Decision Tree')\n",
        "plt.gca().invert_yaxis()\n",
        "st.pyplot(plt)\n",
        "#------------------------------------------------------------\n",
        "st.title('Classification Report for Logistic Regression:')\n",
        "# Import Linear Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "lr_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lr = lr_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "classification_rep_lr = classification_report(y_test, y_pred_lr)\n",
        "conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "\n",
        "# Print evaluation metrics\n",
        "st.write(f'Accuracy (Logistic Regression): {accuracy_lr}')\n",
        "st.write('Classification Report (Logistic Regression):')\n",
        "st.write(classification_rep_lr)\n",
        "st.write('Confusion Matrix (Logistic Regression):')\n",
        "st.write(conf_matrix_lr)\n",
        "\n",
        "# Optional: Feature Importance (coefficients)\n",
        "feature_importances_lr = pd.DataFrame(lr_clf.coef_[0],\n",
        "                                       index=feature_columns,\n",
        "                                       columns=['Importance']).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Print feature importances\n",
        "st.write('Feature Importances (Logistic Regression):')\n",
        "st.write(feature_importances_lr)\n",
        "\n",
        "# Visualize Feature Importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importances_lr.index, feature_importances_lr['Importance'], color='lightgreen')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importances from Logistic Regression')\n",
        "plt.gca().invert_yaxis()\n",
        "st.pyplot(plt)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6fE93kZo7W4",
        "outputId": "6bbaa11d-fd54-4d51-8b9d-392dff72f109"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.42.42.183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " This command reads a CSV file located at '/content/Big-Data/Data/xAPI-Edu-Data.csv' and loads it into a pandas DataFrame named 'data'.\n"
      ],
      "metadata": {
        "id": "IEXhLhiwFnp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "UQX1afxD56mX",
        "outputId": "3314d1a6-7e7d-42f7-a5af-27cef462a6f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.42.42.183:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.77s\n",
            "your url is: https://shy-spoons-search.loca.lt\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n",
            "\n",
            "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n",
            "\n",
            "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n",
            "\n",
            "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n",
            "\n",
            "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n",
            "\n",
            "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n",
            "\n",
            "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n",
            "\n",
            "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n",
            "\n",
            "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n",
            "\n",
            "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n",
            "\n",
            "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n",
            "\n",
            "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "\n",
            "/content/app.py:585: RuntimeWarning:\n",
            "\n",
            "More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}